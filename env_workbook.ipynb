{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import packages\n",
    "\n",
    "#Kspice\n",
    "import sys\n",
    "sys.path.append(r\"C:\\Program Files (x86)\\Kongsberg\\K-Spice\\bin64\") #add to path to allow kspice import\n",
    "import kspice # if import error, check correct python version (3.11)\n",
    "\n",
    "#Basic functionality\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#ML\n",
    "import torch \n",
    "\n",
    "from enviroment import Sim\n",
    "from DQN import DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize simulator\n",
    "\n",
    "project_path = r\"C:\\Appl\\K-Spice-Projects\\Kristin_v23\" #Specify path to downloaded project.\n",
    "_ = kspice.Simulator(project_path) #Create instance of project\n",
    "timeline = _.activate_timeline(\"Engineering\") # Select the avaliable engineering timeline\n",
    "app = \"Topside\" # We only make changes to the topside module NOTE: From software we can #deactivate Wells and Power in ESS model, can this be done from python? (If it increases speed)\n",
    "timeline.initialize() #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Kristin model, parameters and initial conditions\n",
    "\n",
    "timeline.load_model(\"KristinMaria_master\") #Load model\n",
    "timeline.load_parameters(\"KristinMaria_master\") # load fixed parameters\n",
    "timeline.load_initial_condition(\"KristinMaria_master\") # Load initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Sim(timeline, app) # Create env instance\n",
    "env.import_variables(\"xl_tester1.xlsx\") #Import variables of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.check_df(\"Global KPI\") #Check random column in dataframe. i.e. correct labeling in correspondance to simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.timeline.set_speed(10) #Set speed of simulator, this value is what we wish for not necesarly what we get\n",
    "env.timeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0367207917313415"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.timeline.achieved_speed #Here you can see actual speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset(pause = True) # We get reset the process simulator by pausing it and again fetching the initial conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    \"\"\" To ensure the model explores new spaces we will sometimes choose actions randomly. If not random we choose the action which result in the highest expected reward. \n",
    "    Choosing a random action will decay exponientially throughout learning. \n",
    "    \"\"\"\n",
    "    EPS_START = 0.9\n",
    "    EPS_END = 0.05\n",
    "    EPS_DECAY = 1000\n",
    "\n",
    "    global steps_done\n",
    "    sample = np.random.rand(1)\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * np.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            return policy_net(state).max(1).indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor(env.sample(\"action\"), dtype=torch.float32) # random action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the future we want the state at a specific form \n",
    "import itertools\n",
    "state_flat = np.array(list(itertools.chain.from_iterable(state)))\n",
    "state_tensor = torch.tensor(state_flat, dtype=torch.float32).unsqueeze(0) # make torch tensor and add axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1., -1., -1.,  1.,  1.,  1.,  1.,  0.,  1.,  1.])\n"
     ]
    }
   ],
   "source": [
    "steps_done = 0\n",
    "action = select_action(state_tensor) # select action (up, down, stay) for each controller by a factor of 1.\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S23FIT1014 has reached its setpoint 12.993580287770818 [m3/h]\n",
      "[0, 1, 2, 3, 4, 5, 7, 8, 9]\n",
      "S23FIT1107 has reached its setpoint 30.02628758619506 [m3/h]\n",
      "[0, 1, 2, 3, 4, 5, 7, 9]\n",
      "S23TT1004 has reached its setpoint 29.094156591058947 [C]\n",
      "[1, 2, 3, 4, 5, 7, 9]\n",
      "S23LT1066 has reached its setpoint 42.09956626047431 [%]\n",
      "[1, 2, 3, 4, 7, 9]\n",
      "S23TT1034 has reached its setpoint 29.09475188628727 [C]\n",
      "[1, 3, 4, 7, 9]\n",
      "S23LT1006 has reached its setpoint 44.099918043728465 [%]\n",
      "[3, 4, 7, 9]\n",
      "Timeout reached\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((array([2.89829813e+01, 4.39508368e+01, 2.89016007e+01, 4.52237861e+01,\n",
       "         1.85913401e+01, 4.19254276e+01, 1.29328170e+01, 3.65216359e-05,\n",
       "         2.98805423e+01, 1.75633582e-03]),\n",
       "  array([ 7.36596831e+01, -1.88161242e+06, -8.18061287e+05, -1.12106806e+06]),\n",
       "  array([29.        , 44.        , 29.        , 45.        , 21.        ,\n",
       "         42.        , 13.        , 13.        , 30.00000191, 26.        ])),\n",
       " 1,\n",
       " None,\n",
       " False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation, reward, terminated, truncated = env.step(action) # Add changes to model and use a polling function to check for updates, timeout currently as 5 min to reach new setpoint with arg: 0.1 precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.timeline.pause() # Pause simulator"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
